{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "For the purposes of this project, we'll be using data about New York City public schools, which can be found [here](https://data.cityofnewyork.us/browse?category=Education).\n",
    "\n",
    "One of the most controversial issues in the U.S. educational system is the efficacy of standardized tests, and whether they're unfair to certain groups. Given our prior knowledge of this topic, investigating the correlations between SAT scores and demographics might be an interesting angle to take. We could correlate SAT scores with factors like race, gender, income, and more.\n",
    "\n",
    "The SAT, or Scholastic Aptitude Test, is an exam that U.S. high school students take before applying to college. Colleges take the test scores into account when deciding who to admit, so it's fairly important to perform well on it.\n",
    "\n",
    "The test consists of three sections, each of which has 800 possible points. The combined score is out of 2,400 possible points (while this number has changed a few times, the data set for our project is based on 2,400 total points). Organizations often rank high schools by their average SAT scores. The scores are also considered a measure of overall school district quality.\n",
    "\n",
    "New York City makes its [data on high school SAT scores](https://data.cityofnewyork.us/Education/2012-SAT-Results/f9bf-2cp4) available online, as well as the [demographics for each high school](https://data.cityofnewyork.us/Education/2014-2015-DOE-High-School-Directory/n3p6-zve2).\n",
    "\n",
    "Unfortunately, combining both of the data sets won't give us all of the demographic information we want to use. We'll need to supplement our data with other sources to do our full analysis.\n",
    "\n",
    "The same website has several related data sets covering demographic information and test scores. Here are the links to all of the data sets we'll be using:\n",
    "\n",
    "- [SAT scores by school](https://data.cityofnewyork.us/Education/2012-SAT-Results/f9bf-2cp4) - SAT scores for each high school in New York City\n",
    "- [School attendance](https://data.cityofnewyork.us/Education/2010-2011-School-Attendance-and-Enrollment-Statist/7z8d-msnt) - Attendance information for each school in New York City\n",
    "- [Class size](https://data.cityofnewyork.us/Education/2010-2011-Class-Size-School-level-detail/urz7-pzb3) - Information on class size for each school\n",
    "- [AP test results](https://data.cityofnewyork.us/Education/2010-AP-College-Board-School-Level-Results/itfs-ms3e) - Advanced Placement (AP) exam results for each high school (passing an optional AP exam in a particular subject can earn a student college credit in that subject)\n",
    "- [Graduation outcomes](https://data.cityofnewyork.us/Education/2005-2010-Graduation-Outcomes-School-Level/vh2h-md7a) - The percentage of students who graduated, and other outcome information\n",
    "- [Demographics](https://data.cityofnewyork.us/Education/2006-2012-School-Demographics-and-Accountability-S/ihfw-zy9j) - Demographic information for each school\n",
    "- [School survey](https://data.cityofnewyork.us/Education/2011-NYC-School-Survey/mnz3-dyi8) - Surveys of parents, teachers, and students at each school\n",
    "\n",
    "All of these data sets are interrelated. We'll need to combine them into a single data set before we can find correlations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before we move into coding, we'll need to do some background research. A thorough understanding of the data will help us avoid costly mistakes, such as thinking that a column represents something other than what it does. Background research will also give us a better understanding of how to combine and analyze the data.\n",
    "\n",
    "In this case, we'll want to research:\n",
    "\n",
    "- [New York City](https://en.wikipedia.org/wiki/New_York_City)\n",
    "- [The SAT](https://en.wikipedia.org/wiki/SAT)\n",
    "- [Schools in New York City](https://en.wikipedia.org/wiki/List_of_high_schools_in_New_York_City)\n",
    "- [Our data](https://data.cityofnewyork.us/browse?category=Education)\n",
    "\n",
    "We can learn a few different things from these resources. For example:\n",
    "\n",
    "- Only high school students take the SAT, so we'll want to focus on high schools.\n",
    "- New York City is made up of five boroughs, which are essentially distinct regions.\n",
    "- New York City schools fall within several different school districts, each of which can contains dozens of schools.\n",
    "- Our data sets include several different types of schools. We'll need to clean them so that we can focus on high schools only.\n",
    "- Each school in New York City has a unique code called a DBN, or district borough number.\n",
    "- Aggregating data by district will allow us to use the district mapping data to plot district-by-district differences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_files = [\"ap_2010.csv\",\"class_size.csv\",\"demographics.csv\",\"graduation.csv\",\"hs_directory.csv\",\"sat_results.csv\"]\n",
    "data = {}\n",
    "for f in data_files:\n",
    "    d = pd.read_csv('schools/{0}'.format(f))\n",
    "    key_name = f.replace('.csv','')\n",
    "    data[key_name] = d"
   ]
  },
  {
   "source": [
    "What we're mainly interested in is the SAT data set, which corresponds to the dictionary key sat_results. This data set contains the SAT scores for each high school in New York City. We eventually want to correlate selected information from this data set with information in the other data sets.\n",
    "\n",
    "Let's explore sat_results to see what we can discover. Exploring the dataframe will help us understand the structure of the data, and make it easier for us to analyze it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['sat_results'].head())"
   ]
  },
  {
   "source": [
    "We can make a few observations based on this output:\n",
    "\n",
    "The DBN appears to be a unique ID for each school.\n",
    "- We can tell from the first few rows of names that we only have data about high schools.\n",
    "- There's only a single row for each high school, so each DBN is unique in the SAT data.\n",
    "- We may eventually want to combine the three columns that contain SAT scores -- SAT Critical Reading Avg. Score, SAT Math Avg. - Score, and SAT Writing Avg. Score -- into a single column to make the scores easier to analyze.\n",
    "\n",
    "Given these observations, let's explore the other data sets to see if we can gain any insight into how to combine them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "    print(data[key].head())"
   ]
  },
  {
   "source": [
    "We can make some observations based on the first few rows of each one.\n",
    "\n",
    "- Each data set appears to either have a DBN column, or the information we need to create one. That means we can use a DBN column to combine the data sets. First we'll pinpoint matching rows from different data sets by looking for identical DBNs, then group all of their columns together in a single data set.\n",
    "- Some fields look interesting for mapping -- particularly Location 1, which contains coordinates inside a larger string.\n",
    "Some of the data sets appear to contain multiple rows for each school (because the rows have duplicate DBN values). That means weâ€™ll have to do some preprocessing to ensure that each DBN is unique within each data set. If we don't do this, we'll run into problems when we combine the data sets, because we might be merging two rows in one data set with one row in another data set.\n",
    "\n",
    "Before we proceed with the merge, we should make sure we have all of the data we want to unify. We mentioned the survey data earlier (survey_all.txt and survey_d75.txt), but we didn't read those files in because they're in a slightly more complex format."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_survey = pd.read_csv('schools/survey_all.txt', delimiter='\\t', encoding='windows-1252')\n",
    "d75_survey = pd.read_csv('schools/survey_d75.txt', delimiter='\\t', encoding='windows-1252')\n",
    "survey = pd.concat([all_survey,d75_survey], axis=0)\n",
    "print(survey.head())"
   ]
  },
  {
   "source": [
    "There are two immediate facts that we can see in the data:\n",
    "\n",
    "- There are over 2000 columns, nearly all of which we don't need. We'll have to filter the data to remove the unnecessary ones. Working with fewer columns will make it easier to print the dataframe out and find correlations within it.\n",
    "- The survey data has a dbn column that we'll want to convert to uppercase (DBN). The conversion will make the column name consistent with the other data sets.\n",
    "\n",
    "First, we'll need to filter the columns to remove the ones we don't need. Luckily, there's a data dictionary at the original data download location. The dictionary tells us what each column represents. Based on our knowledge of the problem and the analysis we're trying to do, we can use the data dictionary to determine which columns to use."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey['DBN'] = survey['dbn']\n",
    "cols = [\"DBN\", \"rr_s\", \"rr_t\", \"rr_p\", \"N_s\", \"N_t\", \"N_p\", \"saf_p_11\", \"com_p_11\", \"eng_p_11\", \"aca_p_11\", \"saf_t_11\", \"com_t_11\", \"eng_t_11\", \"aca_t_11\", \"saf_s_11\", \"com_s_11\", \"eng_s_11\", \"aca_s_11\", \"saf_tot_11\", \"com_tot_11\", \"eng_tot_11\", \"aca_tot_11\"]\n",
    "survey = survey.loc[:,cols]\n",
    "data['survey'] = survey \n",
    "print(survey.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hs_directory']['DBN'] = data['hs_directory']['dbn']\n",
    "\n",
    "def pad_csd(num):\n",
    "    return str(num).zfill(2)\n",
    "\n",
    "data['class_size']['padded_csd'] = data['class_size']['CSD'].apply(pad_csd)\n",
    "data['class_size']['DBN'] = data['class_size']['padded_csd'] + data['class_size']['SCHOOL CODE']\n",
    "print(data['class_size'].head())"
   ]
  },
  {
   "source": [
    "Now we're almost ready to combine our data sets. Before we do, let's take some time to calculate variables that will be useful in our analysis. We've already discussed one such variable -- a column that totals up the SAT scores for the different sections of the exam. This will make it much easier to correlate scores with demographic factors because we'll be working with a single number, rather than three different ones.\n",
    "\n",
    "Before we can generate this column, we'll need to convert the SAT Math Avg. Score, SAT Critical Reading Avg. Score, and SAT Writing Avg. Score columns in the sat_results data set from the object (string) data type to a numeric data type. We can use the pandas.to_numeric() method for the conversion. If we don't convert the values, we won't be able to add the columns together.\n",
    "\n",
    "It's important to pass the keyword argument errors=\"coerce\" when we call pandas.to_numeric(), so that pandas treats any invalid strings it can't convert to numbers as missing values instead.\n",
    "\n",
    "After we perform the conversion, we can use the addition operator (+) to add all three columns together."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n",
    "\n",
    "for c in cols:\n",
    "    data['sat_results'][c] = pd.to_numeric(data['sat_results'][c], errors='coerce')\n",
    "data['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\n",
    "print(data['sat_results']['sat_score'].head())"
   ]
  },
  {
   "source": [
    "Next, we'll want to parse the latitude and longitude coordinates for each school. This will enable us to map the schools and uncover any geographic patterns in the data. The coordinates are currently in the text field Location 1 in the hs_directory data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_lat(loc):\n",
    "    coords = re.findall('\\(.+\\)', loc)\n",
    "    lat = coords[0].split(',')[0].replace('(','')\n",
    "    return lat\n",
    "\n",
    "data['hs_directory']['lat'] = data['hs_directory']['Location 1'].apply(find_lat)\n",
    "\n",
    "print(data['hs_directory'].head())"
   ]
  },
  {
   "source": [
    "On the last screen, we parsed the latitude from the Location 1 column. Now we'll just need to do the same for the longitude.\n",
    "\n",
    "Once we have both coordinates, we'll need to convert them to numeric values. We can use the pandas.to_numeric() function to convert them from strings to numbers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef find_lon(loc):\n",
    "    coords = re.findall(\"\\(.+\\)\", loc)\n",
    "    lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n",
    "    return lon\n",
    "\n",
    "data[\"hs_directory\"][\"lon\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lon)\n",
    "\n",
    "data[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\n",
    "data[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")\n",
    "\n",
    "print(data[\"hs_directory\"].head())"
   ]
  }
 ]
}